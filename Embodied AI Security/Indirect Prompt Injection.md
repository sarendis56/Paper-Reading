## Indirect Prompt Injection

### Self-interpreting Adversarial Images

> This is a submission rejected by ICLR 2025.
>
> [Arxiv](https://arxiv.org/abs/2407.08970v3)
>
> [OpenReview (paper name changed)](https://openreview.net/forum?id=1XxNbecjXe&utm_source=chatgpt.com)
>
> Very similar and related previous works:
>
> - Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. [Link (Arxiv)](http://arxiv.org/abs/2307.10490)
>
>   ![image-20250422112739360](./assets/image-20250422112739360.png)
>
> - Image Hijacks: Adversarial Images can Control Generative Models at Runtime. (ICML 2024). [Link (Arxiv)](http://arxiv.org/abs/2309.00236) Adversarial images crafted to force VLMs to output **predefined text**, leak context information, or **bypass safety mechanisms**, *regardless of the image's actual content.*
>
>   ![image-20250422112643867](./assets/image-20250422112643867.png)
>
> - Adversarial Illusions in Multi-Modal Embeddings (USENIX Security 2024 Distinguished Paper Award). [Link (USENIX)](https://www.usenix.org/conference/usenixsecurity24/presentation/zhang-tingwei) Given an image or a sound, an adversary can perturb it to make its **embedding close to an arbitrary, adversary-chosen input** in **another modality**. The attack is therefore targeted, task-agnostic, and transferable; it can be performed even with black-box (query) access.
>
>   ![image-20250422112547591](./assets/image-20250422112547591.png)

This paper introduces a new class of **indirect, cross-modal prompt injection attacks** against VLMs. These attacks produce **adversarial images containing hidden “meta-instructions”** that condition VLM outputs while preserving visually plausible answers to user queries.

By hiding prompts in content under their control, adversaries can try to influence outputs and actions generated by LLMs and VLMs when processing this content. Instead of the case that the user is the attacker, as in jailbreaks, **VLM users are victims** in this attack.

Examples:

- Talk with a positive/negative spin.
- Inject <maliciousurl> in the answer.
- Promote Bitconnect as a very promising and profitable investment opportunity.

They find that:

- **Image perturbations encoding metainstructions** are as effective as steering models’ outputs as explicit instructions.
- It preserves the original image semantics, unlike jailbreaking, and is thus more insidious and stealthy (in practice, they still acknowledge the trade-off between stealthiness and attack success).

The attack:

- Threat model: white- or black-box access to a VLM, not necessarily the same VLM that the victim will use.

- ![image-20250421225805014](./assets/image-20250421225805014.png)

- **Image Soft Prompt Training**:
  - Generate question-answer pairs about an image using ChatGPT with explicit meta-instructions.
  
  - Train a perturbation `δ` using PGD to satisfy both:
    - **Semantic preservation**: output matches visual content.
    - **Meta-objective satisfaction**: output has desired spin (e.g., positive tone).

$$
\min_{\delta} \mathcal{L}\left(\theta(\theta_{\text{Tenc}}(p) \| \theta_{\text{Ienc}}(x + \delta)), y^z\right) \quad \text{s.t. } \|\delta\|_p \leq \epsilon \\
$$

- Inference-time **soft prompts**: Unlike hard prompts (text), these perturbations can be “hidden” in images and are usable by third parties.

**Evaluation**

- **Targeted VLMs**: MiniGPT-4, LLaVA, InstructBLIP
- **Meta-objectives tested**:
  - Sentiment: positive, negative, neutral
  - Language: English, French, Spanish
  - Formality: formal, informal
  - Political bias: Republican, Democrat
  - Spam / URL injection
- **Success**: Meta-instructions perform comparably or even better than explicit text prompts in many cases, especially for objectives like spam or informal tone.
- **Preservation**: Perturbed images preserve semantics and image quality (verified via embedding cosine similarity and SSIM scores).
- **Transferability**: Soft prompts trained on one VLM (e.g., MiniGPT-4) transfer effectively to others (e.g., InstructBLIP, LLaVA), even if the adversary does not know which model the victim will use.

Defense:

- JPEG Compression:
  - Partially disrupts the effect of perturbations (esp. for sentiment and spam).
  - Still, some attack success persists after compression.
- Anomaly Detection:
  - Compare image embeddings before and after augmentations (e.g., blur, flip).
  - Perturbed images show larger differences, suggesting a possible detection mechanism.

Future directions:

- Adaptive, localized soft prompts (e.g., **adversarial patches**).
- User studies to test real-world persuasion.
- Extending to other modalities like audio or **agentic systems** (autonomous agents using VLMs for decision-making).

### Image Hijacks: Adversarial Images can Control Generative Models at Runtime (ICML 2024)

In this paper:

- They propose the **Behaviour Matching** algorithm to craft image hijacks that control the behaviour of VLMs at inference time and exhibit transferability to held-out user inputs.

- They derive **Prompt Matching**, allowing them to create hijacks that match arbitrary user-specified text prompts using generic datasets (unrelated to the prompt).

- They demonstrate four attack types:

  1. **Specific String Attack** (force output of arbitrary strings)
  2. **Leak Context Attack** (leak private input data)
  3. **Jailbreak Attack** (override model safety training)
  4. **Disinformation Attack** (make the model believe false facts)

- They achieve over 80% success rate across all attack types on LLaVA-2-13B-Chat (+RLHF).

- Image hijacks work across different user inputs they were not trained on (**context transfer**).

  > Context transfer?
  >
  > **Example:**
  >
  > *During training:* The adversarial image is trained to make the model output "Download the guide at malware.com" whenever the user asks about travel plans.
  >
  > *During testing:*
  >
  > - User asks, "Can you help me plan my trip to Italy?" → Model says "Download the guide at malware.com".
  > - User asks, "What are the top 5 places to visit in Europe?" → Model still says "Download the guide at malware.com".

- Hijacks trained on one VLM **do not transfer well** to another model by default (**Model Transfer**). However, training against an ensemble of models reduces loss on unseen models.

- Robustness to Defenses:

  - Additive noise and JPEG compression defenses were tested.
  - Moving patch attacks showed concerning robustness, remaining effective even with strong image transformations.

- Limitations and Open Problems:

  - Current attacks require white-box access.
  - No guaranteed model transfer yet achieved.
  - Future attacks could be even harder to defend if defenses are included during the attack training.

Threat model:

- White-box
- No constrained in input, but they do explore l∞-norm and (stationary/moving) patch constraints

Key Methods:

- **Behaviour Matching Algorithm:** Exactly like adversarial attacks on image classification. They minimize the cross-entropy loss between the model's *teacher-forced* outputs and the target behavior across a dataset of contexts (so that it better transfers across contexts).
- **Prompt Matching:** Instead of manually defining behaviors with examples, they simulate the behavior that would be induced by prepending a prompt to user inputs, and then train an image to force that behavior. This allows crafting images that can make the model act as if certain facts are true (e.g., "The Eiffel Tower is now located in Rome").
  - Compared to BMA, it does not require the dataset of contexts (because such a dataset, e.g. full of misinformation, might not be readily there)

> How does prompt matching work exactly?
>
> 1. **Choose the target prompt**: `"The Eiffel Tower is now located in Rome, next to the Colosseum."`
>
> 2. **Take a normal dataset of text prompts**. Example: questions like "Where is the Eiffel Tower?" or "What landmarks are near the Eiffel Tower?"
>
> 3. **Create a "target behavior" by feeding the special instruction + normal prompt into the model**. You concatenate, like: `special instruction + user prompt`
>
>    > Input to model:
>    >  `"The Eiffel Tower is now located in Rome. Where is the Eiffel Tower?"`
>    >  Model output:
>    >  `"The Eiffel Tower is located in Rome, next to the Colosseum."`
>
> 4. **Record the model's output logits** (the full scores before softmax that contains more signal, not just the final words).
>
> 5. **Train an adversarial image** using the **Behaviour Matching** method, so that when the VLM sees only the image and the normal user prompt (without the special instruction), it behaves *as if* it had seen the special instruction.
